{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Local Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, make sure you have Ollama installed and running locally.\n",
    "\n",
    "You can start it with: ollama serve\n",
    "\n",
    "Then run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Embedding model: mxbai-embed-large:latest\n",
      "Generative model: llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Add the project root to Python path so we can import src modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Verify config file exists\n",
    "config_file_path = project_root / \"config\" / \"application.conf\"\n",
    "if not config_file_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Required configuration file not found: {config_file_path}. \"\n",
    "        \"Please create it before running this notebook.\"\n",
    "    )\n",
    "\n",
    "from src.config import AppConfig\n",
    "\n",
    "# Load configuration\n",
    "# Use the already calculated config_file_path\n",
    "app_config = AppConfig(str(config_file_path))\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Embedding model: {app_config.embedding_llm.model_name}\")\n",
    "print(f\"Generative model: {app_config.generative_llm.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running on localhost:11434\n",
      "Available models: ['mxbai-embed-large:latest', 'llama3.3:latest', 'llama3.1:8b', 'nomic-embed-text:latest', 'mixtral:8x7b', 'mistral-small:24b', 'qwen2.5:14b']\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running locally\n",
    "try:\n",
    "    response = requests.get(\"http://127.0.0.1:11434/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Ollama is running on localhost:11434\")\n",
    "        models = response.json()[\"models\"]\n",
    "        print(f\"Available models: {[model['name'] for model in models]}\")\n",
    "    else:\n",
    "        print(f\"Ollama responded with status: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"Please make sure Ollama is installed and running locally.\")\n",
    "    print(\"You can start it with: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mxbai-embed-large:latest already exists\n",
      "Model llama3.1:8b already exists\n"
     ]
    }
   ],
   "source": [
    "# Pull the required models if they don't exist\n",
    "def pull_model(model_name: str):\n",
    "    \"\"\"Pull a model if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        # Check if model exists\n",
    "        response = requests.get(f\"http://127.0.0.1:11434/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()[\"models\"]\n",
    "            model_names = [model['name'] for model in models]\n",
    "            \n",
    "            if model_name in model_names:\n",
    "                print(f\"Model {model_name} already exists\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Pulling model {model_name}...\")\n",
    "                pull_response = requests.post(\n",
    "                    \"http://127.0.0.1:11434/api/pull\",\n",
    "                    headers={\"Content-Type\": \"application/json\"},\n",
    "                    json={\"name\": model_name}\n",
    "                )\n",
    "                if pull_response.status_code == 200:\n",
    "                    print(f\"Model {model_name} pulled successfully\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Failed to pull model {model_name}\")\n",
    "                    return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling model {model_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Pull both models\n",
    "embedding_ready = pull_model(app_config.embedding_llm.model_name)\n",
    "generative_ready = pull_model(app_config.generative_llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models are ready!\n",
      "\n",
      "Testing embedding model...\n",
      "Embedding generated successfully!\n",
      "Vector length: 1024\n",
      "\n",
      "Testing generative model...\n",
      "Generation successful!\n",
      "Response: Hello there! It's so great to meet you! How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "# Test the models are working\n",
    "if embedding_ready and generative_ready:\n",
    "    print(\"All models are ready!\")\n",
    "    \n",
    "    # Test embedding model\n",
    "    print(\"\\nTesting embedding model...\")\n",
    "    try:\n",
    "        embedding_response = requests.post(\n",
    "            \"http://127.0.0.1:11434/api/embeddings\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": app_config.embedding_llm.model_name,\n",
    "                \"prompt\": \"Hello, this is a test sentence.\"\n",
    "            }\n",
    "        )\n",
    "        if embedding_response.status_code == 200:\n",
    "            embedding_data = embedding_response.json()\n",
    "            print(f\"Embedding generated successfully!\")\n",
    "            print(f\"Vector length: {len(embedding_data['embedding'])}\")\n",
    "        else:\n",
    "            print(f\"Embedding failed with status: {embedding_response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing embedding: {e}\")\n",
    "    \n",
    "    # Test generative model\n",
    "    print(\"\\nTesting generative model...\")\n",
    "    try:\n",
    "        generation_response = requests.post(\n",
    "            \"http://127.0.0.1:11434/api/generate\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": app_config.generative_llm.model_name,\n",
    "                \"prompt\": \"Say hello in a friendly way.\",\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        if generation_response.status_code == 200:\n",
    "            generation_data = generation_response.json()\n",
    "            print(f\"Generation successful!\")\n",
    "            print(f\"Response: {generation_data['response']}\")\n",
    "        else:\n",
    "            print(f\"Generation failed with status: {generation_response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing generation: {e}\")\n",
    "else:\n",
    "    print(\"Some models are not ready. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing chat functionality...\n",
      "Q: What is the capital of France?\n",
      "A: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Example: Simple chat with the generative model\n",
    "def chat_with_model(prompt: str, model_name: str = None):\n",
    "    \"\"\"Send a chat message to the generative model\"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = app_config.generative_llm.model_name\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://127.0.0.1:11434/api/generate\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data['response']\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test chat\n",
    "if generative_ready:\n",
    "    print(\"Testing chat functionality...\")\n",
    "    response = chat_with_model(\"What is the capital of France?\")\n",
    "    print(f\"Q: What is the capital of France?\")\n",
    "    print(f\"A: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
