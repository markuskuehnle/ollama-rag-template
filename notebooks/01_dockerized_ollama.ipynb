{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Dockerized Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path so we can import src modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Verify config file exists\n",
    "config_file_path = project_root / \"config\" / \"application.conf\"\n",
    "if not config_file_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Required configuration file not found: {config_file_path}. \"\n",
    "        \"Please create it before running this notebook.\"\n",
    "    )\n",
    "\n",
    "from src.environment.environment import (\n",
    "    setup_environment,\n",
    "    teardown_environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2025-07-23 22:10:06,209] [environment.py:23]: Setup test environment\n",
      "[INFO] [2025-07-23 22:10:06,219] [container.py:172]: Pulling image testcontainers/ryuk:0.8.1\n",
      "[INFO] [2025-07-23 22:10:07,253] [container.py:198]: Container started: b42c1a627331\n",
      "[INFO] [2025-07-23 22:10:07,265] [waiting_utils.py:52]: Waiting for container <Container: b42c1a627331> with image testcontainers/ryuk:0.8.1 to be ready ...\n",
      "[INFO] [2025-07-23 22:10:08,277] [container.py:172]: Pulling image ollama/ollama:0.5.13\n",
      "[INFO] [2025-07-23 22:10:08,371] [container.py:198]: Container started: a3468bc31911\n",
      "[INFO] [2025-07-23 22:10:09,404] [ollama.py:28]: Pulling mxbai-embed-large:latest model (this may take a while)...\n",
      "[INFO] [2025-07-23 22:10:09,959] [ollama.py:34]: Done pulling mxbai-embed-large:latest\n",
      "[INFO] [2025-07-23 22:10:09,960] [ollama.py:35]: Response: <Response [200]>\n",
      "[INFO] [2025-07-23 22:10:09,973] [container.py:172]: Pulling image ollama/ollama:0.5.13\n",
      "[INFO] [2025-07-23 22:10:10,100] [container.py:198]: Container started: 660c638e04ae\n",
      "[INFO] [2025-07-23 22:10:11,125] [ollama.py:28]: Pulling llama3.1:8b model (this may take a while)...\n",
      "[INFO] [2025-07-23 22:10:11,708] [ollama.py:34]: Done pulling llama3.1:8b\n",
      "[INFO] [2025-07-23 22:10:11,708] [ollama.py:35]: Response: <Response [200]>\n",
      "Ollama containers started successfully!\n"
     ]
    }
   ],
   "source": [
    "# Start the Ollama containers\n",
    "embedding_container, generative_container = setup_environment()\n",
    "print(\"Ollama containers started successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding container status: 200\n",
      "Available models: ['mxbai-embed-large:latest', 'llama3.3:latest', 'llama3.1:8b', 'nomic-embed-text:latest', 'mixtral:8x7b', 'mistral-small:24b', 'qwen2.5:14b']\n",
      "Generative container status: 200\n",
      "Available models: ['llama3.1:8b']\n"
     ]
    }
   ],
   "source": [
    "# Test the containers are working\n",
    "import requests\n",
    "\n",
    "# Test embedding container (port 11434)\n",
    "try:\n",
    "    response = requests.get(\"http://127.0.0.1:11434/api/tags\")\n",
    "    print(f\"Embedding container status: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()[\"models\"]\n",
    "        print(f\"Available models: {[model['name'] for model in models]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to embedding container: {e}\")\n",
    "\n",
    "# Test generative container (port 11435)\n",
    "try:\n",
    "    response = requests.get(\"http://127.0.0.1:11435/api/tags\")\n",
    "    print(f\"Generative container status: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()[\"models\"]\n",
    "        print(f\"Available models: {[model['name'] for model in models]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to generative container: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [2025-07-23 22:10:12,114] [environment.py:48]: Tearing down the test environment, nothing do to\n",
      "Containers stopped and cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Clean up containers when done\n",
    "if 'embedding_container' in locals():\n",
    "    embedding_container.stop()\n",
    "if 'generative_container' in locals():\n",
    "    generative_container.stop()\n",
    "teardown_environment()\n",
    "print(\"Containers stopped and cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
